{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Import FBP\n",
    "sys.path.append('../FBPConvNet/')\n",
    "from FBPConvNet import FBPConvNet, Discriminator\n",
    "\n",
    "sys.path.append('../')\n",
    "from net_utils import get_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    return torch.Tensor(data).unsqueeze(1)\n",
    "def target_ones(N,gpu=False):\n",
    "    if GPU:\n",
    "        return torch.ones(N,1).cuda()\n",
    "    else:\n",
    "        return torch.ones(N,1)\n",
    "def target_zeros(N,gpu=False):\n",
    "    if GPU:\n",
    "        return torch.zeros(N,1).cuda()\n",
    "    else:\n",
    "        return torch.zeros(N,1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to data\n",
    "pathtodata = '../EllipseGeneration/RandomLineEllipses15.hdf5'\n",
    "dataset_size = 200\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(pathtodata,'r')\n",
    "print(f['ellip/training_data'].shape)\n",
    "fakeinput = preprocess(f['ellip/training_data'][0:dataset_size])\n",
    "fakelabels = preprocess(f['ellip/training_labels'][0:dataset_size])\n",
    "reallabels = preprocess(f['ellip/training_labels'][dataset_size:2*dataset_size])\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "faketrainset = TensorDataset(fakeinput,fakelabels)\n",
    "#realset = TensorDataset(reallabels)\n",
    "\n",
    "faketrainloader = DataLoader(faketrainset,batch_size=batch_size,shuffle=True)\n",
    "realtrainloader = DataLoader(reallabels, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(realtrainloader)\n",
    "realiter = iter(realtrainloader)\n",
    "for i in range(len(realtrainloader)):\n",
    "    data = realiter.next()\n",
    "    print(data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Input 1 x 256 x 256 -> 64 x 256 x 256\n",
    "        self.conv1 = nn.Conv2d(1,64,3,padding=1,stride=1)    \n",
    "        self.batch1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # -> 128 x 128 x 128\n",
    "        self.conv2 = nn.Conv2d(64,128,7,padding=3,stride=2)\n",
    "        self.batch2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # -> 256 x 32 x 32 -> 256 x 22 x 22\n",
    "        self.conv3 = nn.Conv2d(128,256,5,padding=2,stride=2)\n",
    "        self.batch3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256,256,7,padding=3,stride=3)\n",
    "        self.batch4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # -> 512 x 8 x 8\n",
    "        self.conv5 = nn.Conv2d(256,512,5,padding=2,stride=3)\n",
    "        self.batch5 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # -> 1024 x 1 x 1\n",
    "        self.conv6 = nn.Conv2d(512,1024,5,padding=2,stride=3)\n",
    "        self.batch6 = nn.BatchNorm2d(1024)\n",
    "        self.conv7 = nn.Conv2d(1024,1024,3,padding=1,stride=3)\n",
    "        #self.batch7 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        # Decision layers\n",
    "        self.conv8 = nn.Conv2d(1024,1024,1)\n",
    "        self.conv9 = nn.Conv2d(1024,1024,1)\n",
    "        self.conv10 = nn.Conv2d(1024,1,1)\n",
    "        \n",
    "        # Non-Linear Activations\n",
    "        self.leaky = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.conv2_drop = nn.Dropout2d(p=.2)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Generate Features\n",
    "        x = self.leaky(self.batch1(self.conv1(x)))\n",
    "        x = self.leaky(self.batch2(self.conv2(x)))\n",
    "        x = self.leaky(self.batch3(self.conv3(x)))\n",
    "        x = self.leaky(self.batch4(self.conv4(x)))\n",
    "        x = self.leaky(self.batch5(self.conv5(x)))\n",
    "        x = self.leaky(self.batch6(self.conv6(x)))\n",
    "        x = self.leaky(self.conv7(x))\n",
    "        \n",
    "        # Decision Layers\n",
    "        x = self.leaky(self.conv2_drop(self.conv8(x)))\n",
    "        x = self.leaky(self.conv2_drop(self.conv9(x)))\n",
    "        x = self.sigmoid(self.conv10(x))\n",
    "        return x[:,:,0,0]\n",
    "#         x1_1 = self.batch1(self.elu(self.conv1_1(x)))\n",
    "#         x1_2 = self.batch1(self.elu(self.conv1_2(x1_1)))\n",
    "#         x1_3 = self.batch1(self.elu(self.conv1_2(x1_2)))\n",
    "#         x1 = self.maxpool(x1_3)\n",
    "        \n",
    "#         x2_1 = self.batch2(self.elu(self.conv2_1(x1)))\n",
    "#         x2_2 = self.batch2(self.elu(self.conv2_2(x2_1)))\n",
    "#         x2 = self.maxpool(x2_2)\n",
    "        \n",
    "#         x3_1 = self.batch3(self.elu(self.conv3_1(x2)))\n",
    "#         x3_2 = self.batch3(self.elu(self.conv3_2(x3_1)))\n",
    "#         x3 = self.maxpool(x3_2)\n",
    "        \n",
    "#         x4_1 = self.batch4(self.elu(self.conv4_1(x3)))\n",
    "#         x4_2 = self.batch4(self.elu(self.conv4_2(x4_1)))\n",
    "\n",
    "#         x5_1 = self.deconv5(x4_2)\n",
    "#         x5_2 = torch.cat((x3_2,x5_1),1)\n",
    "#         x5_3 = self.batch3(self.elu(self.conv5_1(x5_2)))\n",
    "#         x5 = self.batch3(self.elu(self.conv5_2(x5_3)))\n",
    "        \n",
    "#         x6_1 = self.deconv6(x5)\n",
    "#         x6_2 = torch.cat((x2_2,x6_1),1)\n",
    "#         x6_3 = self.batch2(self.elu(self.conv6_1(x6_2)))\n",
    "#         x6 = self.batch2(self.elu(self.conv6_2(x6_3)))\n",
    "        \n",
    "#         x7_1 = self.deconv7(x6)\n",
    "#         x7_2 = torch.cat((x1_3,x7_1),1)\n",
    "#         x7_3 = self.batch1(self.elu(self.conv7_1(x7_2)))\n",
    "#         x7 = self.batch1(self.elu(self.conv7_2(x7_3)))\n",
    "        \n",
    "#         x8 = self.conv8(x7)\n",
    "#         y = x8 + x\n",
    "        \n",
    "#         return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Discriminator().cuda()\n",
    "x = Variable(fakeinput)\n",
    "print(x.size())\n",
    "out = net.forward(x)\n",
    "print(out.size())\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Softplus()\n",
    "input_ = Variable(torch.randn(2))\n",
    "print(input_)\n",
    "print(m(input_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GANs(G, D, faketrainloader, realtrainloader, num_epochs=500, GPU=False,\n",
    "              weightpath='./weights/',save_epoch=50,saveweights=True):\n",
    "    # Create output directory\n",
    "    weightpath = os.path.join(weightpath,get_datetime())\n",
    "    os.makedirs(weightpath)\n",
    "    logpath = os.path.join(weightpath,'log.txt')\n",
    "    \n",
    "    with open(logpath, \"wt\") as text_file:\n",
    "        print('Epoch\\tD Loss\\tG Loss\\tEpoch Time\\tTotal Time',file=text_file)\n",
    "\n",
    "    num_data = len(realtrainloader)*realtrainloader.batch_size \n",
    "    d_losses = np.zeros(num_epochs)\n",
    "    g_losses = np.zeros(num_epochs)\n",
    "\n",
    "    # Accumulate log text\n",
    "    logtxt = ''\n",
    "    \n",
    "    # Determine minibatch size\n",
    "    minibatch = max(1,int(len(realtrainloader))/10)\n",
    "    \n",
    "    # Define Loss Function/Optimizer\n",
    "    bceloss = nn.BCELoss()\n",
    "    mseloss = nn.MSELoss()\n",
    "\n",
    "    d_optimizer = optim.Adam(D.parameters(), lr=0.0002)\n",
    "    g_optimizer = optim.Adam(G.parameters(), lr=0.0002)\n",
    "\n",
    "    \n",
    "    G.train()\n",
    "    trainstart = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        # Collect loss information\n",
    "        d_epoch_loss = 0.0\n",
    "        g_epoch_loss = 0.0\n",
    "        d_running_loss = 0.0\n",
    "        g_running_loss = 0.0\n",
    "        \n",
    "        epochstart = time.time()\n",
    "\n",
    "        fakeiter = iter(faketrainloader)\n",
    "        realiter = iter(realtrainloader)\n",
    "        Giter = iter(faketrainloader)\n",
    "        for batch_index in range(len(realtrainloader)):\n",
    "            ## prepare data\n",
    "            truelabels = realiter.next()\n",
    "            fakeinput, _ = fakeiter.next()\n",
    "            batch_size = truelabels.size(0)\n",
    "\n",
    "\n",
    "            if GPU:\n",
    "                truelabels = truelabels.cuda()\n",
    "                fakeinput = fakeinput.cuda()\n",
    "    #             fakelabel = fakelabel.cuda()\n",
    "            d_real_data = Variable(truelabels)\n",
    "            d_gen_input = Variable(fakeinput)\n",
    "            d_fake_data = G(d_gen_input).detach() # detach to avoid training G on these labels\n",
    "            \n",
    "            ## Train D\n",
    "            d_optimizer.zero_grad()\n",
    "\n",
    "            # Train D on real\n",
    "            d_real_decision = D(d_real_data)\n",
    "            d_real_error = bceloss(d_real_decision, Variable(target_ones(batch_size,GPU)))\n",
    "            d_real_error.backward()\n",
    "\n",
    "            # Train D on fake\n",
    "\n",
    "            d_fake_decision = D(d_fake_data)\n",
    "            d_fake_error = bceloss(d_fake_decision, Variable(target_zeros(batch_size,GPU))) \n",
    "            d_fake_error.backward()\n",
    "            d_optimizer.step()\n",
    "            d_loss = d_real_error+d_fake_error\n",
    "            \n",
    "            d_running_loss += d_loss.data[0]\n",
    "            d_losses[epoch] += d_loss.data[0]\n",
    "            \n",
    "        \n",
    "            ## Train G\n",
    "            g_fake_input, g_fake_label = Giter.next()\n",
    "            batch_size = g_fake_input.size(0)\n",
    "\n",
    "            if GPU:\n",
    "                g_fake_input = g_fake_input.cuda()\n",
    "                g_fake_label = g_fake_label.cuda()\n",
    "\n",
    "            gen_input = Variable(g_fake_input)\n",
    "            g_fake_data = G(gen_input)\n",
    "  \n",
    "            g_optimizer.zero_grad()\n",
    "\n",
    "            dg_fake_decision = D(g_fake_data)\n",
    "            g_loss = (10**-3)*bceloss(dg_fake_decision, Variable(target_ones(batch_size,GPU)))\n",
    "            g_loss +=  mseloss(g_fake_data,Variable(g_fake_label))\n",
    "\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            g_running_loss += g_loss.data[0]\n",
    "            g_losses[epoch] += g_loss.data[0]\n",
    "            \n",
    "            # print statistics\n",
    "            if batch_index % minibatch == 0:\n",
    "                print('\\t[%d, %5d] D loss: %.3f, G loss: %.3f, %.3f seconds elapsed' %\n",
    "                      (epoch + 1, batch_index + 1, d_running_loss / minibatch, \n",
    "                       g_running_loss/minibatch, time.time() - epochstart))\n",
    "                d_running_loss = 0.0\n",
    "                g_running_loss = 0.0\n",
    "        # Record epoch statistics\n",
    "        epochend = time.time()        \n",
    "        print('Epoch %d Training Time: %.3f seconds\\nTotal Elapsed Time: %.3f seconds' %\n",
    "               (epoch+1, epochend-epochstart,epochend-trainstart))\n",
    "        \n",
    "        # log losses\n",
    "        d_losses[epoch] /= num_data\n",
    "        g_losses[epoch] /= num_data\n",
    "        logtxt += '%i\\t%f\\%f\\t%f\\t%f\\n' % (epoch+1,d_losses[epoch], g_losses[epoch],\n",
    "                                           epochend-epochstart,epochend-trainstart)\n",
    "\n",
    "        \n",
    "        # Save weights\n",
    "        if (epoch % save_epoch == 0 or epoch == num_epochs-1):\n",
    "            if saveweights:\n",
    "                d_outpath = os.path.join(weightpath,'D_epoch_'+str(epoch+1)+'.weights')\n",
    "                g_outpath = os.path.join(weightpath,'G_epoch_'+str(epoch+1)+'.weights')\n",
    "                D = D.cpu()\n",
    "                G = G.cpu()\n",
    "                torch.save(D.state_dict(),d_outpath)\n",
    "                torch.save(G.state_dict(),g_outpath)\n",
    "\n",
    "                if GPU:\n",
    "                    D = D.cuda()\n",
    "                    G = G.cuda()\n",
    "            \n",
    "            # write loss to logfile\n",
    "            with open(logpath, \"at\") as text_file:\n",
    "                print(logtxt[:-2],file=text_file)\n",
    "                logtxt = ''\n",
    "\n",
    "    print('Finished Training')\n",
    "    return d_losses,g_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator()\n",
    "G = FBPConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[1,     1] D loss: 0.069, G loss: 0.010, 3.016 seconds elapsed\n",
      "\t[1,    21] D loss: 1.313, G loss: 0.020, 21.829 seconds elapsed\n",
      "\t[1,    41] D loss: 0.700, G loss: 0.010, 40.758 seconds elapsed\n",
      "\t[1,    61] D loss: 0.624, G loss: 0.008, 59.681 seconds elapsed\n",
      "\t[1,    81] D loss: 0.403, G loss: 0.007, 78.721 seconds elapsed\n",
      "\t[1,   101] D loss: 0.954, G loss: 0.011, 97.797 seconds elapsed\n",
      "\t[1,   121] D loss: 0.210, G loss: 0.005, 116.852 seconds elapsed\n",
      "\t[1,   141] D loss: 0.906, G loss: 0.008, 135.871 seconds elapsed\n",
      "\t[1,   161] D loss: 0.608, G loss: 0.013, 154.950 seconds elapsed\n",
      "\t[1,   181] D loss: 0.081, G loss: 0.011, 174.036 seconds elapsed\n",
      "Epoch 1 Training Time: 192.121 seconds\n",
      "Total Elapsed Time: 192.121 seconds\n",
      "\t[2,     1] D loss: 0.000, G loss: 0.001, 0.967 seconds elapsed\n",
      "\t[2,    21] D loss: 1.642, G loss: 0.006, 19.986 seconds elapsed\n",
      "\t[2,    41] D loss: 0.514, G loss: 0.006, 39.010 seconds elapsed\n",
      "\t[2,    61] D loss: 0.012, G loss: 0.014, 58.037 seconds elapsed\n",
      "\t[2,    81] D loss: 2.681, G loss: 0.008, 77.057 seconds elapsed\n",
      "\t[2,   101] D loss: 1.407, G loss: 0.004, 96.065 seconds elapsed\n",
      "\t[2,   121] D loss: 1.188, G loss: 0.003, 115.058 seconds elapsed\n",
      "\t[2,   141] D loss: 1.053, G loss: 0.003, 134.097 seconds elapsed\n",
      "\t[2,   161] D loss: 1.261, G loss: 0.006, 153.135 seconds elapsed\n",
      "\t[2,   181] D loss: 0.602, G loss: 0.007, 172.150 seconds elapsed\n",
      "Epoch 2 Training Time: 190.235 seconds\n",
      "Total Elapsed Time: 383.436 seconds\n",
      "\t[3,     1] D loss: 0.007, G loss: 0.000, 0.950 seconds elapsed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-868eb5adfbdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0md_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_GANs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfaketrainloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrealtrainloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGPU\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGPU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-5675b64407cc>\u001b[0m in \u001b[0;36mtrain_GANs\u001b[1;34m(G, D, faketrainloader, realtrainloader, num_epochs, GPU, weightpath, save_epoch, saveweights)\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mg_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[0mg_running_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m             \u001b[0mg_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "num_epochs = 10\n",
    "\n",
    "GPU = True\n",
    "if GPU:\n",
    "    D = D.cuda()\n",
    "    G = G.cuda()\n",
    "d_losses, g_losses = train_GANs(G,D,faketrainloader,realtrainloader,num_epochs=num_epochs,GPU=GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.plot(d_losses)\n",
    "plt.title('Discriminator Losses')\n",
    "plt.subplot(122)\n",
    "plt.plot(g_losses)\n",
    "plt.title('Generator Losses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeiter = iter(faketrainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,x = fakeiter.next()\n",
    "\n",
    "GPU = True\n",
    "if GPU:\n",
    "    xhat = G(Variable(y.cuda()))\n",
    "    xhat = xhat.cpu().data\n",
    "else:\n",
    "    xhat = G(Variable(y)).data\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "mse =torch.mean((y[0,0,:,:]-x[0,0,:,:])**2)\n",
    "plt.imshow(y[0,0,...].numpy())\n",
    "plt.title('Input, mse=%.3f'%(mse))\n",
    "plt.axis('off')\n",
    "\n",
    "mse =torch.mean((xhat[0,0,:,:]-x[0,0,:,:])**2)\n",
    "plt.subplot(132)\n",
    "plt.imshow(xhat[0,0,...].numpy())\n",
    "plt.title('Output, mse=%.3f'%(mse))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(x[0,0,...].numpy())\n",
    "plt.title('GT')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(y.numpy()),np.max(y.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = G.cpu()\n",
    "torch.save(G.state_dict(),'Gmse_100.weights')\n",
    "D = D.cpu()\n",
    "torch.save(D.state_dict(),'Dmse_100.weights')\n",
    "\n",
    "if GPU:\n",
    "    G = G.cuda()\n",
    "    D = D.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
